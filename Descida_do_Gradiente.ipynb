{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Descida do Gradiente.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyODpyx9OEtvxcFU2XR84LoB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heberht/deep-learning-keras-tf-tutorial/blob/master/Descida_do_Gradiente.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsiE_iS1Zu55"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np # para álgebra linear np.random.seed(0) # para consistência nos resultados \n",
        "dados = pd.DataFrame()\n",
        "dados['x'] = np.linspace(-10,10,100)\n",
        "dados['y'] = 5 + 3*dados['x'] + np.random.normal(0,3,100)\n",
        "\n",
        "# define a função custo \n",
        "def L(y, y_hat):\n",
        "  return ((y, y_hat) ** 2).sum()\n",
        "\n",
        "# implementa regressão linear com gradiente descendente class linear_regr(object):\n",
        "\n",
        "def __init__(self, learning_rate=0.0001, training_iters=50):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.training_iters = training_iters\n",
        "\n",
        "def fit(self, X_train, y_train):\n",
        "\n",
        "        # formata os dados         \n",
        "        if len(X_train.values.shape) < 2:\n",
        "            X = X_train.values.reshape(-1,1)\n",
        "        X = np.insert(X, 0, 1, 1)\n",
        "\n",
        "        # inicia os parâmetros com pequenos valores aleatórios                  \n",
        "        self.w_hat = np.random.normal(0,5, size = X[0].shape)\n",
        "\n",
        "        for _ in range(self.training_iters):\n",
        "\n",
        "            gradient = np.zeros(self.w_hat.shape) # inicia o gradiente \n",
        "            # computa o gradiente com informação de todos os pontos          \n",
        "        for point, yi in zip(X, y_train):\n",
        "                gradient +=  (point * self.w_hat - yi) * point\n",
        "\n",
        "            # multiplica o gradiente pela taxa de aprendizado \n",
        "\n",
        "                gradient *= self.learning_rate \n",
        "                self.w_hat -= gradient            \n",
        "                  \n",
        "def predict(self, X_test):\n",
        "        # formata os dados         \n",
        "        if len(X_test.values.shape) < 2:\n",
        "            X = X_test.values.reshape(-1,1)\n",
        "        X = np.insert(X, 0, 1, 1)\n",
        "\n",
        "        return np.dot(X, self.w_hat)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKEZD7bzeTW4"
      },
      "source": [
        "np.random.seed(23)\n",
        "\n",
        "# implementa regressão linear com gradiente descendente estocástico class linear_regr(object):\n",
        "\n",
        "def __init__(self, learning_rate=0.0001, batch_size=5, training_iters=50):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.training_iters = training_iters\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "def fit(self, X_train, y_train, plot=False):\n",
        "\n",
        "        # formata os dados         \n",
        "        if len(X_train.values.shape) < 2:\n",
        "            X = X_train.values.reshape(-1,1)\n",
        "            X = np.insert(X, 0, 1, 1)\n",
        "\n",
        "        # inicia os parâmetros com pequenos valores aleatórios         # (nosso chute razoável)         \n",
        "        self.w_hat = np.random.normal(0,5, size = X[0].shape)\n",
        "\n",
        "        for i in range(self.training_iters):\n",
        "\n",
        "            # cria os mini-lotes             \n",
        "            \n",
        "            offset = (i * self.batch_size) % (y_train.shape[0] - self.batch_size)\n",
        "            batch_X = X[offset:(offset + self.batch_size), :]\n",
        "            batch_y = y_train[offset:(offset + self.batch_size)]\n",
        "\n",
        "            gradient = np.zeros(self.w_hat.shape) # inicia o gradiente \n",
        "            # atualiza o gradiente com informação dos pontos do lote             for point, yi in zip(batch_X, batch_y):\n",
        "            gradient +=  (point * self.w_hat - yi) * point\n",
        "\n",
        "            gradient *= self.learning_rate\n",
        "            self.w_hat -= gradient\n",
        "\n",
        "def predict(self, X_test):\n",
        "        # formata os dados         \n",
        "        if len(X_test.values.shape) < 2:\n",
        "            X = X_test.values.reshape(-1,1)\n",
        "            X = np.insert(X, 0, 1, 1)\n",
        "\n",
        "        return np.dot(X, self.w_hat) \n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtCmhuXzexp4"
      },
      "source": [
        "np.random.seed(23)\n",
        "\n",
        "# implementa regressão linear # com gradiente descendente estocástico e momento class linear_regr(object):\n",
        "\n",
        "def __init__(self, learning_rate=0.0001, batch_size=5, gamma=0.9, training_iters=50):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.training_iters = training_iters\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.w_hat = np.random.normal(0,5, size = X[0].shape)\n",
        "\n",
        "def fit(self, X_train, y_train, plot=False):\n",
        "\n",
        "        # formata os dados     \n",
        "\n",
        "         if len(X_train.values.shape) < 2:\n",
        "            X = X_train.values.reshape(-1,1)\n",
        "            X = np.insert(X, 0, 1, 1)\n",
        "            self.w_hat = np.random.normal(0,5, size = X[0].shape)\n",
        "            velocidade =  np.zeros(self.w_hat.shape) # inicia a velocidade \n",
        "\n",
        "           \n",
        "            for i in range(self.training_iters):\n",
        "\n",
        "            # cria os mini-lotes             \n",
        "              offset = (i * self.batch_size) % (y_train.shape[0] - self.batch_size)\n",
        "              batch_X = X[offset:(offset + self.batch_size), :]\n",
        "              batch_y = y_train[offset:(offset + self.batch_size)]\n",
        "\n",
        "            gradient = np.zeros(self.w_hat.shape) # inicia o gradiente \n",
        "            # atualiza o gradiente com informação dos pontos do lote             \n",
        "            for point, yi in zip(batch_X, batch_y):\n",
        "                gradient +=  (point * self.w_hat - yi) * point\n",
        "\n",
        "            gradient *= self.learning_rate\n",
        "\n",
        "            # atualiza a velocidade             \n",
        "            velocidade = (velocidade * self.gamma) + gradient\n",
        "            \n",
        "            self.w_hat -= velocidade # atualiza a os parâmetros \n",
        "def predict(self, X_test):\n",
        "        # formata os dados         \n",
        "        if len(X_test.values.shape) < 2:\n",
        "            X = X_test.values.reshape(-1,1)\n",
        "        X = np.insert(X, 0, 1, 1)\n",
        "\n",
        "        return np.dot(X, self.w_hat) \n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "z23Xb_h-gouW",
        "outputId": "70349b64-58b1-45a5-c2ec-ea75fa7a9a29"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(dir(tf.feature_column))\n",
        "\n",
        "\n",
        "x, y = dados['x'].values, dados['y'].values # dados \n",
        "# Monta a estrutura tf \n",
        "# valores iniciais shape \n",
        "\n",
        "W_hat = tf.Variable(tf.random.normal([1], 0, 5))\n",
        "b_hat = tf.Variable(tf.zeros([1]))\n",
        "\n",
        "# modelo \n",
        "\n",
        "y_hat = W_hat * x + b_hat\n",
        "\n",
        "# Função custo def cost():\n",
        "loss = tf.reduce_mean(tf.square(y_hat - y))\n",
        "\n",
        "\n",
        "# otimizador \n",
        "train = tf.keras.optimizers.Adam().minimize(loss(), var_list=[W_hat,b_hat]\n",
        "\n",
        "sess = tf.Session()\n",
        " # para rodar a estrutura \n",
        "sess.run(tf.global_variables_initializer()) # inicia variáveis \n",
        " # roda 200 iterações de treino for step in range(200):\n",
        "sess.run(optimizer)\n",
        "\n",
        "\n",
        "w_final, b_final = sess.run([W_hat, b_hat])\n",
        "print('Após treinamento, w_hat = %.2f e w_hat = %.2f' % (w_final[0], b_final[0]))\n",
        "\n",
        "sess.close()        "
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-119-3e2fe651138d>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    sess = tf.Session()\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}